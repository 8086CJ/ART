---
title: "Summarizer Tutorial"
sidebarTitle: "Summarizer"
description: "Train a summarizer model to outperform Sonnet 4 and GPT-4.1."
icon: "list"
---

Most SOTA models are already trained to condense long documents into short summaries. However, not every summary is created equal.

In this tutorial, we're going to train a summarizer that excels at filtering useful information from a document and cutting out the fluff. To skip ahead and see the results of a prior training run, check out the [blog post](https://openpipe.ai/blog/summary-rl). Otherwise, please enjoy this tutorial!

<Info>

Reading time: <b>45 min</b>

Training time: <b>4 hours</b>

Total cost: <b>$22</b>

</Info>

## Step 1: Clone the starter repo

To get started, clone [Summary-RL](https://github.com/OpenPipe/Summary-RL/), which contains the following pieces of our RL pipeline:

- The agent's environment
- The reward function
- Some training examples

## Step 2: Install dependencies and set up environment variables

Follow steps 1-3 in the [Getting Started](https://github.com/OpenPipe/Summary-RL/?tab=readme-ov-file#getting-started) section to set up your local laptop for training. We highly recommend at least setting your `WANDB_API_KEY` to track agent performance over time, though it is not necessary to train your model. If you plan to benchmark your model or host it after training, you'll also need to set your AWS access keys (instructions linked in [step 3](https://github.com/OpenPipe/Summary-RL/?tab=readme-ov-file#3-set-up-optional-environment-variables-found-in-envexample)).

## Step 1: Create Your OpenPipe Account

If you don't already have one, create an account with OpenPipe at https://app.openpipe.ai/. You can sign up with GitHub, so you don't need to remember an extra password.

## Step 2: Find Your Project API Key

In order to capture your calls and fine-tune a model on them, we need an API key to authenticate you and determine which project to store your logs under.

<Note>
  When you created your account, a project was automatically configured for you
  as well. Find its API key at https://app.openpipe.ai/settings.
</Note>

## Step 3: Record Training Data (Optional)

If you don't have any training data, you can record it by integrating the OpenPipe SDK or using the OpenPipe Proxy. If you already have a dataset, you can skip this step!

<CardGroup cols={2}>
  <Card
    title="Installing the SDK"
    icon="code"
    iconType="duotone"
    href="/getting-started/openpipe-sdk"
  ></Card>
  <Card
    title="Using the OpenPipe Proxy"
    icon="layer-group"
    href="/features/request-logs/logging-requests#proxy"
  ></Card>
</CardGroup>

## Step 4: Prepare a Dataset

Datasets are the core of OpenPipe. They store your training data, and allow you to fine-tune and evaluate models on it. To learn more about datasets, check out the [Datasets](/features/datasets/overview) page.

Datasets can be populated in two ways:

1. [Uploading external data](/features/datasets/uploading-data)
2. [Importing request logs](/features/datasets/importing-logs)

If you already have a dataset, we recommend uploading it as a starting point. Otherwise, make sure you set up request logging in step 3!

## Step 5: Fine Tune a Model

Once your dataset has been created and populated, you can fine-tune models on it. Follow the [fine-tuning quickstart](/features/fine-tuning/quick-start) guide to kick off your first fine-tuning run.

<Frame>![](/images/features/fine-tuning/fine-tune-modal.png)</Frame>

We recommend training several models of varying sizes and configurations to determine the best one for your use case. If you have questions on this step, please reach out to us at [support@openpipe.ai](mailto:support@openpipe.ai)!

## Step 6: Evaluate Your Model

Once your model (or models) have been fine-tuned, you can evaluate them. To learn more about evaluating models, check out the [Evaluations](/features/evaluations/overview) page.

## Step 7: Deploy Your Model

By default, your model will be automatically hosted on OpenPipe's cloud infrastructure. Additionally, you can export and deploy any of our open-weight models on your own cloud.

Good luck! If you have any questions, don't hesitate to reach out!
